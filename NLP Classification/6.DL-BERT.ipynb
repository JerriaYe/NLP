{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT有点难以理解，Transformer之前也没有基础，本身代码也不是很强，datawhale给的代码也很长，因此这块学得有点艰难。\n",
    "\n",
    "梳理一下看的一些公号文章 or 博客的有关BERT的心得。\n",
    "## 相关知识\n",
    "### 隐马尔科夫模型(HMM)\n",
    "HMM是非常适合用于序列标注问题的。HMM模型引入了马尔科夫假设，即**T时刻的状态仅仅与前一时刻的状态相关。**\n",
    "\n",
    "### 条件随机场(CRF)\n",
    "但是，在序列标注任务中，**当前时刻的状态，应该同该时刻的前后的状态均相关**。于是，在很多序列标注任务中，引入了条件随机场。\n",
    "\n",
    "### 循环神经网络(RNN)\n",
    "能够“记忆”序列输入的历史信息，从而能够较好的对整个序列进行语义建模。\n",
    "RNN虽然理论上可以很漂亮的解决序列数据的训练，但是它也像DNN一样有梯度消失的问题，当序列很长的时候问题尤其严重。虽然同选择合适的激活函数等方法能够一定程度的减轻该问题。但人们往往更青睐于使用RNN的变种。\n",
    "\n",
    "### LSTM\n",
    "\n",
    "LSTM在原本RNN的基础上增加了CEC的内容，CEC保证了误差以常数的形式在网络中流动，这部分通过引入细胞状态C来体现。\n",
    "并且，为了解决输入和输出在参数更新时的矛盾，在CEC的基础上添加3个门使得模型变成非线性的，就可以调整不同时序的输出对模型后续动作的影响。\n",
    "\n",
    "### NLP中Attention机制\n",
    "在信息处理过程中，对不同的内容分配不同的注意力权重。\n",
    "\n",
    "#### 特征抽取器Tranformer\n",
    "Transformer中最重要的特点就是引入了Attention，特别是Multi-Head Attention。作为一个序列输入的特征抽取器，其编码能力强大，没有明显的缺点。短期内难以看到可以匹敌的竞争对手。\n",
    "#### transformerXL\n",
    "TransformerXL是Transformer一种非常重要的改进，通过映入Recurrence机制和相对位置编码，增强了Transformer在长文本输入上的特征抽取能力。\n",
    "\n",
    "TransformerXL学习的依赖关系比RNN长80%，比传统Transformer长450%，在短序列和长序列上都获得了更好的性能，并且在评估阶段比传统Transformer快1800+倍。\n",
    "#### GPT及GPT2.0\n",
    "GPT，特别是GPT2.0是一个大型无监督语言模型，能够生产连贯的文本段落，在许多语言建模基准上取得了 SOTA 表现。而且该模型在没有任务特定训练的情况下，能够做到初步的阅读理解、机器翻译、问答和自动摘要。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT\n",
    "BERT，全称是Pre-training of Deep Bidirectional Transformers for Language Understanding。注意其中的每一个词都说明了BERT的一个特征。\n",
    "\n",
    "Pre-training说明BERT是一个预训练模型，通过前期的大量语料的无监督训练，为下游任务学习大量的先验的语言、句法、词义等信息。\n",
    "\n",
    "Bidirectional 说明BERT采用的是双向语言模型的方式，能够更好的融合前后文的知识。\n",
    "\n",
    "Transformers说明BERT采用Transformers作为特征抽取器。\n",
    "\n",
    "Deep说明模型很深，base版本有12层，large版本有24层。\n",
    "\n",
    "总的来说，BERT是一个用Transformers作为特征抽取器的深度双向预训练语言理解模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
